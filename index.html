<!DOCTYPE html>
<html>
<head>
	<meta charset='utf-8' />
	<meta http-equiv="X-UA-Compatible" content="chrome=1" />
	<meta name="description" content="IO Benchmark : Comparison of IO methods for OS X and Linux" />
	<link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">
	<title>IO Benchmark</title>
</head>

<body>

<!-- HEADER -->
<div id="header_wrap" class="outer">
	<header class="inner">
		<a id="forkme_banner" href="https://github.com/adityaramesh/io_benchmark">View on GitHub</a>

		<h1 id="project_title">IO Benchmark</h1>
		<h2 id="project_tagline">Comparison of IO methods for OS X and Linux</h2>

			<section id="downloads">
				<a class="zip_download_link" href="https://github.com/adityaramesh/io_benchmark/zipball/master">Download this project as a .zip file</a>
				<a class="tar_download_link" href="https://github.com/adityaramesh/io_benchmark/tarball/master">Download this project as a tar.gz file</a>
			</section>
	</header>
</div>

<!-- MAIN CONTENT -->
<div id="main_content_wrap" class="outer">
<section id="main_content" class="inner">
<h1><a name="motivation" class="anchor" href="#motivation"><span class="octicon octicon-link"></span></a>Motivation</h1>

<p>I work on machine learning projects that involve serializing and deserializing
large amounts of data. The IO can often be the chief bottleneck in the programs
that I write. Naturally, I wanted to determine the fastest methods for
sequentially reading, writing, and copying files on OS X and Linux. This series
of benchmarks was written towards this end.</p>

<p>This page briefly describes the methodology, and my observations based on the
results from the two machines on which I ran the benchmarks. I also quickly
threw up a <a
href="https://docs.google.com/a/adityaramesh.com/forms/d/1h2ogikCYrW3oCFjMJhWSoyZQ0NeyP3759JRquyjh-YM/viewform">Google
form</a> so people can run the benchmarks on different systems and share their
results. The form submissions are <a
href="https://docs.google.com/spreadsheets/d/12ZfXUtRwItcUwgKDt9HF5mIdEIdFbgXTVL5RjaqHAL0/edit?usp=sharing">publicly
viewable.</a></p>

<p>Before proceeding, I would like to warn you that I am by no means a systems
expert. I have tried to be cautious when writing the benchmarks, but it is very
possible that I got a few details wrong, or missed some others entirely. Feel
free to communicate to me any comments or suggestions that you have about the
project.</p>

<h1><a name="overview" class="anchor" href="#overview"><span class="octicon octicon-link"></span></a>Overview</h1>

<p>All of the benchmarks rely on the test files generated by
<code>tools/make_data.rb</code>. The implementations of the benchmarks can be
found in the <code>src</code> directory. Some common IO wrapper functions, and a
copy of the <a href="https://github.com/adityaramesh/ccbase">CCBase</a> headers,
are found in the <code>include</code> directory.<p>

<p>Each benchmark tests a series of IO methods that perform the same task. Each
IO method is evaluated <code>num_trials</code> times for each file size in the
range defined in <code>tools/test_read.sh</code> or
<code>tools/test_write.sh</code>, where <code>num_trials</code> is defined in
<code>include/configuration.hpp</code>. For each IO method, the test function
defined in <code>include/test.hpp</code> prints out the following
information:</p>

<ul>
<li>The file size, in megabytes.</li>
<li>The name of the IO method used.</li>
<li>The mean completion time, in milliseconds.</li>
<li>The standard deviation, in milliseconds.</li>
</ul>

<p>You can access the raw results for the two machines on which I ran the
benchmarks <a
href="https://github.com/adityaramesh/io_benchmark/tree/master/results">here</a>.

<p>Each method used in the read benchmark counts the occurrences of
<code>needle</code> (set to <code>0xFF</code> in
<code>include/configuration.hpp</code>) as a check for correctness. To mitigate
the effects of caching, the test function also purges the page cache after each
method evaluation (see <code>purge_cache</code> in
<code>include/io_common.hpp</code> for how this is done). The write benchmark
writes a specified number of randomly-generated bytes to a destination file. The
last benchmark, which tests copying, does not need much explanation.</p>

<h1><a name="methods" class="anchor" href="#methods"><span class="octicon octicon-link"></span></a>Methods</h1>

<p>The IO methods can be segregated into two broad categories. Each IO method
either uses a buffer to read or write one chunk at a time within a loop, or uses
a special function to accomplish the same task. Methods in the first class fall
into one of three categories: synchronous IO, POSIX AIO, or lock-free
asynchronous IO using C++ atomics. Examples of IO methods in the second class
are <code>mmap</code>, <code>splice</code>, and <code>sendfile</code>, with the
latter two being Linux-specific. I did not use Linux AIO, because I found
earlier that it <a
href="https://stackoverflow.com/questions/20973754/linux-aio-poor-scaling">scales
very poorly</a>. To my knowledge, Linux AIO only works reliably for raw block
devices.</p>

<p>At first glance, it may not make sense why one would benefit from using
asynchronous IO for these benchmarks. But all three benchmarks perform work that
can be pipelined using a double-buffering scheme. The read benchmark counts the
occurrences of <code>needle</code>, the write benchmark generates random bytes,
and the copy benchmark must read a chunk of data before it can write it.
Using asynchronous double-buffering, we can produce frame <code>n + 1</code>
while consuming frame <code>n</code>. The results show that, even for the simple
tasks performed by the benchmarks, this double-buffering scheme often results in
a speedup over synchronous IO.</p>

<p>Each IO method also uses zero or more of the following optimizations (which,
in many cases, turn out to be pessimizations): direct IO, read ahead advice to
the OS (for reading and copying), and preallocation (for writing and copying).
Sometimes, there is more than one way to perform a particular optimization on
the target platform. In such cases, all possible combinations of the available
choices were tried. I omitted reporting the combinations that generally yielded
slower execution times across the entire range of file sizes.</p>

<h3><a name="read_optimizations" class="anchor" href="#read_optimizations"><span class="octicon octicon-link"></span></a>Read Optimizations</h3>

<p>On OS X, one can disable caching using <code>fcntl</code> with the
<code>F_NOCACHE</code> flag. On Linux, this is done by opening the file using
the <code>O_DIRECT</code> flag. Further information I found online suggested
that the IO buffer address and length should both be multiples of the page size,
and the request length should be a multiple of the file system's block size. I
used <code>posix_memalign</code> to accommodate for these constraints.</p>

<p>OS X offers two different kinds of read ahead optimizations via
<code>fcntl</code>: <code>F_RDAHEAD</code> and <code>F_RDADVISE</code>. Based on
available documentation, I gathered that the <code>F_RDADVISE</code> and
<code>F_RDAHEAD</code> are analogous to the <code>FADV_WILLNEED</code> and
<code>FADV_SEQUENTIAL</code> flags on Linux, respectively. The
<code>FADV_WILLNEED</code> flag initiates a non-blocking read of the specified
region into the page cache. Experiment has shown that <code>F_RDADVISE</code>
has a similar effect on OS X. On Linux, the <code>FADV_SEQUENTIAL</code> flag
doubles the size of the read ahead buffer.  Presumably, the
<code>F_RDAHEAD</code> flag does something similar on OS X, but the manual was
sparse on details.</p>

<h3><a name="write_optimizations" class="anchor" href="#write_optimizations"><span class="octicon octicon-link"></span></a>Write Optimizations</h3>

<p>On both OS X and Linux, one can preallocate a file before writing to it, in
order to accelerate IO. As with the other optimizations, OS X exposes this
interface via <code>fcntl</code> and the <code>F_PREALLOCATE</code> flag. Linux
has the <code>fallocate</code> and <code>ftruncate</code> functions, which do
slightly different things. Using the <code>posix_fallocate</code> function on
Linux is inadvisable, because glibc emulates the behavior (very inefficiently!)
even if the underlying file system does not support the operation. The
<code>fallocate</code> function throws <code>EOPNOTSUPP</code> in this case, so
the programmer has the option of falling back to other approaches. Finally, <a
href="https://blog.mozilla.org/tglek/2010/09/09/help-wanted-does-fcntlf_preallocate-work-as-advertised-on-osx/">this</a>
Mozilla blog post warns that on OS X, <code>F_PREALLOCATE</code> need to be
followed by <code>truncate</code> in order to force the data to be written to
the file. I implemented both approaches in the benchmarks.</p>

<h3><a name="copy_optimizations" class="anchor" href="#copy_optimizations"><span class="octicon octicon-link"></span></a>Copy Optimizations</h3>

<p>OS X and Linux both allow you to copy files using a simple read-and-write
loop or <code>mmap</code>. Linux also has the <code>splice</code> and
<code>sendfile</code> functions, which avoid copying data from kernel buffers to
userspace buffers.</p>

<h1><a name="results" class="anchor" href="#results"><span class="octicon octicon-link"></span></a>Results</h1>

<p>Before I discuss the results for the two systems on which I ran the
benchmarks, here is the exhaustive list of read methods that I tried.  All
buffer-based IO methods were evaluated using each of the following buffer sizes:
4 KB, 8 KB, 12 KB, 16 KB, 24 KB, 32 KB, 40 KB, 48 KB, 56 KB, 64 KB, 256 KB, 1024
KB, 4096 KB, 16384 KB, 65536 KB, and 262144 KB.</p>

<pre><code>// OS X:
read_plain
read_nocache
read_rdahead
read_rdadvise
read_aio_nocache
read_aio_rdahead
read_aio_rdadvise
read_async_nocache
read_async_rdahead
read_async_rdadvise
read_mmap_plain
read_mmap_rdahead
read_mmap_rdadvise

// Linux:
read_plain
read_direct
read_fadvise
aio_read_direct
aio_read_fadvise
read_async_plain
read_async_direct
read_async_fadvise
read_mmap_plain
read_mmap_fadvise
</code></pre>

<p>Here is the list of write methods:</p>

<pre><code>// OS X:
write_plain
write_nocache
write_preallocate
write_preallocate_truncate
write_preallocate_truncate_nocache
async_write_preallocate_truncate_nocache
write_mmap

// Linux:
write_plain
write_direct
write_preallocate
write_truncate
write_direct_preallocate
write_direct_truncate
write_async_plain
write_async_direct
write_async_preallocate
write_async_truncate
write_async_direct_preallocate
write_async_direct_truncate
mmap_preallocate
mmap_preallocate_direct
mmap_preallocate
mmap_truncate_direct
</code></pre>

<p>Finally, here is a list of the copy methods. Note that I avoided testing many
combinations of optimizations that I expected to perform poorly based on the
read and write results.</p>

<pre><code>// OS X:
copy_plain
copy_nocache
copy_rdahead_preallocate
copy_rdadvise_preallocate
copy_mmap

// Linux:
copy_plain
copy_direct
copy_preallocate
copy_mmap_plain
copy_mmap_fadvise
copy_splice
copy_splice_preallocate
copy_splice_preallocate_fadvise
copy_splice_fadvise
copy_sendfile
copy_sendfile_preallocate
copy_sendfile_preallocate_fadvise
copy_sendfile_fadvise
</code></pre>

<p>The benchmarks were run on two systems: a mid-2012 Macbook Pro with an SSD,
running OS X 10.9, and a Linux server with a PCIe SSD, running Arch Linux with
kernel version 3.14.4. The Macbook Pro was formatted to HFS+, while the Linux
server was using ext4 for the partition on which the benchmark was run.</p>

<h3><a name="results_macbook_pro" class="anchor" href="#results_macbook_pro"><span class="octicon octicon-link"></span></a>Results: Macbook Pro</h3>

<p>Below, I have tabulated what I found to be the best schemes for IO on the
Macbook Pro. The data from which these conclusions were drawn can be found <a
href="https://github.com/adityaramesh/io_benchmark/tree/master/results/macbook_pro">here.</a></p>

<table>
<tr>
	<th>Task</th>
	<th>IO Type</td>
	<th>File Size</th>
	<th>Method</th>
	<th>Buffer Size</th>
</tr>
<tr>
	<td>Reading</td>
	<td>Synchronous</td>
	<td>&lt; 256 MB</td>
	<td><code>read_rdadvise</code></td>
	<td>4 KB &mdash; 1024 KB</td>
</tr>
<tr>
	<td>Reading</td>
	<td>Synchronous</td>
	<td>&ge; 256 MB</td>
	<td><code>read_rdahead</code></td>
	<td>&ge; 4096 KB</td>
</tr>
<tr>
	<td>Reading</td>
	<td>Asynchronous</td>
	<td>&lt; 128 MB</td>
	<td><code>read_async_rdadvise</code></td>
	<td>Base on workload.</td>
</tr>
<tr>
	<td>Reading</td>
	<td>Asynchronous</td>
	<td>&ge; 128 MB</td>
	<td><code>read_async_rdahead</code></td>
	<td>Base on workload.</td>
</tr>
<tr>
	<td>Writing</td>
	<td>Synchronous</td>
	<td>Any</td>
	<td><code>write_preallocate_truncate_nocache</code></td>
	<td>&ge; 1024 KB</td>
</tr>
<tr>
	<td>Writing</td>
	<td>Asynchronous</td>
	<td>Any</td>
	<td><code>write_async_preallocate_truncate_nocache</code></td>
	<td>Base on workload.</td>
</tr>
<tr>
	<td>Copying</td>
	<td>Any</td>
	<td>Any</td>
	<td><code>copy_mmap</code></td>
	<td>N/A</td>
</tr>
</table>

<p>For reading, there is a point after which it makes sense to stop using
<code>F_RDADVISE</code> and instead use <code>F_RDAHEAD</code>. It would be nice
to find out where this point occurs without actually running the benchmark on
the machine. For writing, the method mentioned performed the fastest for all
file sizes tested. For any given file size, the best asynchronous IO method was
generally faster than the best synchronous IO method. This shows that despite
the modest synchronization overhead, it is still worth it to exploit parallelism
where possible.</p>

<p>For reading and writing, the choice of buffer size does make a modest
difference in performance. It may be worth the time to try out a range of buffer
sizes if you know in advantage that your program will only run on a particular
platform. The difference is much more dramatic when <code>F_NOCACHE</code> is
used for reading, but this approach did not yield good results anyway. For
copying, the fastest method in all cases was a simple method that used
<code>mmap</code> twice and <code>std::copy</code> to transfer the data.</p>

<h3><a name="results_linux_server" class="anchor" href="#results_linux_server"><span class="octicon octicon-link"></span></a>Results: Linux Server</h3>

<p>Below, I have tabulated what I found to be the best schemes for IO on the
Linux server. The data from which these conclusions were drawn can be found <a
href="https://github.com/adityaramesh/io_benchmark/tree/master/results/julie">here.</a></p>

<table>
<tr>
	<th>Task</th>
	<th>IO Type</td>
	<th>File Size</th>
	<th>Method</th>
	<th>Buffer Size</th>
</tr>
<tr>
	<td>Reading</td>
	<td>Synchronous</td>
	<td>Any</td>
	<td><code>read_plain</code></td>
	<td>4 KB &mdash; 256 KB</td>
</tr>
<tr>
	<td>Reading</td>
	<td>Asynchronous</td>
	<td>Any</td>
	<td><code>read_async_plain</code></td>
	<td>Base on workload.</td>
</tr>
<tr>
	<td>Writing</td>
	<td>Any</td>
	<td>&lt; 256 MB</td>
	<td><code>write_preallocate</code></td>
	<td>&ge; 4096 KB</td>
</tr>
<tr>
	<td>Writing</td>
	<td>Any</td>
	<td>&ge; 256 MB</td>
	<td><code>write_mmap</code></td>
	<td>N/A</td>
</tr>
<tr>
	<td>Copying</td>
	<td>Any</td>
	<td>Any</td>
	<td><code>copy_sendfile</code></td>
	<td>N/A</td>
</tr>
</table>

<p>The results for reading on the Linux server were a lot less decisive than
they were on Macbook Pro. Using <code>FADV_SEQUENTIAL</code> sometimes resulted
in a speedup, and other times resulted in performance degradations. There was no
clear tradeoff point in terms of the file size. However, <code>read_plain</code>
(which uses <code>pread</code> in a loop) generally performed well with a buffer
size of 32 KB.</p>

<p>The implementation of <code>mmap</code> on Linux seems to be much more
competitive than the one on OS X. For reading and writing, <code>mmap</code>
generally resulted in poor performance on the Macbook Pro. On the Linux server,
<code>mmap</code> is the preferred method for writing more 256 MB or more at a
time. Finally, <code>splice</code> and <code>sendfile</code> were the clear
winners for copying files. I preferred <code>sendfile</code> to
<code>splice</code>, since the usage is much simpler, and the programmer does
not have to choose a buffer size.</p>

<h1><a name="conclusion" class="anchor" href="#conclusion"><span class="octicon octicon-link"></span></a>Conclusion</h1>

<p>It is difficult to draw conclusions solely from the results I presented
earlier. The best IO method to use for a given file size will certainly vary
based on the type of the storage device, file system, kernel version, and so on.
If more people run the benchmark on different systems and publish their results
using <a
href="https://docs.google.com/a/adityaramesh.com/forms/d/1h2ogikCYrW3oCFjMJhWSoyZQ0NeyP3759JRquyjh-YM/viewform">the
survey</a> I put up, then perhaps some general trends will emerge. Until then, I
hope that these benchmarks will serve as a useful reference for those looking to
optimize IO performance on OS X and Linux. Thanks for reading, and feel free to
send me your comments and suggestions!</p>

</section>
</div>

<!-- FOOTER	-->
<div id="footer_wrap" class="outer">
	<footer class="inner">
		<p class="copyright">IO Benchmark maintained by <a href="https://github.com/adityaramesh">adityaramesh</a></p>
		<p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
	</footer>
</div>

</body>
</html>
